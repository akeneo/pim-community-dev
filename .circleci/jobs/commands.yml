commands:
    set_gcloud_config_dev:
        description: "Authenticate on GCP services and set config and key to be used by other tools that need to authenticate."
        steps:
            - run:
                  name: "Set Gcloud Config."
                  shell: "/bin/bash -eo pipefail"
                  command: |
                      echo ${GCLOUD_SERVICE_KEY_DEV} | gcloud auth activate-service-account --key-file=-
                      gcloud config set project ${GOOGLE_PROJECT_ID}
                      gcloud config set compute/zone ${GOOGLE_COMPUTE_ZONE}
                      gcloud container clusters get-credentials ${GOOGLE_COMPUTE_ZONE} --project=${GOOGLE_PROJECT_ID} --zone=${GOOGLE_COMPUTE_ZONE}
                      echo ${GCLOUD_SERVICE_KEY_DEV} > ${HOME}/gcloud-service-key.json
                      echo 'export GOOGLE_APPLICATION_CREDENTIALS="${HOME}/gcloud-service-key.json"' >> $BASH_ENV
                      export GOOGLE_APPLICATION_CREDENTIALS="${HOME}/gcloud-service-key.json"
                      gcloud auth configure-docker --quiet

    set_gcloud_config_preprod:
        description: "Authenticate on GCP services and set config and key to be used by other tools that need to authenticate."
        steps:
            - run:
                  name: "Set Gcloud Config."
                  shell: "/bin/bash -eo pipefail"
                  command: |
                      echo ${GCLOUD_SERVICE_KEY_PREPROD} | gcloud auth activate-service-account --key-file=-
                      gcloud config set project ${GOOGLE_PROJECT_ID}
                      gcloud config set compute/zone ${GOOGLE_COMPUTE_ZONE}
                      gcloud container clusters get-credentials ${GOOGLE_COMPUTE_ZONE} --project=${GOOGLE_PROJECT_ID} --zone=${GOOGLE_COMPUTE_ZONE}
                      echo ${GCLOUD_SERVICE_KEY_PREPROD} > ${HOME}/gcloud-service-key.json
                      echo 'export GOOGLE_APPLICATION_CREDENTIALS="${HOME}/gcloud-service-key.json"' >> $BASH_ENV
                      export GOOGLE_APPLICATION_CREDENTIALS="${HOME}/gcloud-service-key.json"
                      gcloud auth configure-docker --quiet

    restore_persisted_env_vars:
        description: "Restore env vars that have been persisted by the previous job."
        steps:
            - run:
                  name: Restore persisted env vars
                  command: |
                      echo "Persisted env vars:"
                      cat persisted_env_vars
                      cat persisted_env_vars >> $BASH_ENV

    modify_cluster:
        description: "Modify cluster version if needed"
        steps:
            - run:
                  name: Modify cluster version
                  command: |
                      GOOGLE_COMPUTE_ZONE=${GOOGLE_COMPUTE_ZONE_NEXT}
                      CLUSTER_NAME=${CLUSTER_NAME_NEXT}
                      GOOGLE_CLUSTER_ZONE=${CLUSTER_NAME_NEXT}
                      echo export GOOGLE_COMPUTE_ZONE=${GOOGLE_COMPUTE_ZONE} >> $BASH_ENV
                      echo export CLUSTER_NAME=${CLUSTER_NAME} >> $BASH_ENV
                      echo export GOOGLE_CLUSTER_ZONE=${GOOGLE_CLUSTER_ZONE} >> $BASH_ENV

    install_yq:
        description: "Install yq"
        steps:
            - run:
                  name: Install yq
                  command: |
                      wget https://github.com/mikefarah/yq/releases/download/3.3.1/yq_linux_386
                      sudo mv yq_linux_386 /usr/local/bin/yq
                      echo "e7fa464149a450d068311a244f403757408a745b  /usr/local/bin/yq" > /tmp/checksum
                      sha1sum -c /tmp/checksum
                      sudo chmod +x /usr/local/bin/yq

    change_pim_onboarder_branch_steps:
        description: "Change Onboarder dependency if same branch exists"
        steps:
            - when:
                  condition:
                      not:
                          equal: [master, << pipeline.git.branch >>]
                  steps:
                      - run:
                            name: Update composer.json if same branch exists in PIM Onboarder for EE
                            command: |
                                curl -H "Authorization: token ${GITHUB_TOKEN}" \
                                    -H 'Accept: application/vnd.github.v3.raw' \
                                    --output /dev/null --silent --head --fail \
                                    -L https://api.github.com/repos/akeneo/pim-onboarder/contents/README.md?ref=${CIRCLE_BRANCH} && \
                                    sed -i "s#akeneo/pim-onboarder\": \"^7.0.0#akeneo/pim-onboarder\": \"dev-${CIRCLE_BRANCH}#" composer.json || true

    show_datadog_logs_links:
      description: "Show datadog usefulls links for given PFID"
      parameters:
        pfid:
          type: string
        dd_domain:
          default: "https://app.datadoghq.eu"
          type: string
        display_migration_link:
          default: true
          type: boolean
      steps:
        - run:
            name: DATADOG deployment Livetail logs page
            command: |
              echo "Instance logs :"
              echo "<<parameters.dd_domain>>/logs/livetail?query=kube_namespace%3A<<parameters.pfid>>"
              <<# parameters.display_migration_link >>
              echo ""
              FROM_TS=$(date -d '1 hour ago' '+%s%3N')
              TO_TS=$(date -d '3 hours' '+%s%3N')
              echo "Migration timings :"
              echo "<<parameters.dd_domain>>/logs?query=kube_namespace%3A<<parameters.pfid>>%20component%3Ahook-upgrader%20container_name%3Apim-upgrade%20took&from_ts=${FROM_TS}&to_ts=${TO_TS}&live=false"
              <</ parameters.display_migration_link >>

    get_datadog_migration_logs:
      description: "Get datadog migration logs as json"
      parameters:
        pfid:
          type: string
        dd_domain:
          default: "https://app.datadoghq.eu"
          type: string
        storage_file:
          default: "/tmp/migration-logs.json"
          type: string
      steps:
        - run:
            name: DATADOG migration logs
            command: |
              FROM_TS="$(date -d '1 hour ago' '+%Y-%m-%dT%H:%M:%S%z' | cut -c1-22):00"
              TO_TS="$(date -d '3 hours' '+%Y-%m-%dT%H:%M:%S%z' | cut -c1-22):00"
              LOGS=$(curl --location -H "Content-Type: application/json" -H "DD-API-KEY: ${DATADOG_API_KEY}" -H "DD-APPLICATION-KEY: ${DATADOG_APP_KEY}" --request POST "<<parameters.dd_domain>>/api/v2/logs/events/search" --data-raw '{"filter": {"query": "kube_namespace:'"<<parameters.pfid>>"' component:hook-upgrader container_name:pim-upgrade took","indexes": ["main"],"from": "'"${FROM_TS}"'","to": "'"${TO_TS}"'"},"sort": "timestamp","page": {"limit": 100}}')
              echo ${LOGS} > <<parameters.storage_file>>
              if [[ "${LOGS}" != "" ]]; then
                echo ${LOGS} | jq -r .data[].attributes.message
              fi

    skip_job:
        description: "Skip following steps when turned on"
        parameters:
          skip:
            type: boolean
            description: "boolean to skip following steps."
        steps:
          - run:
              name: skip if asked
              command:   |
                  [ "<< parameters.skip >>" = "true" ] && circleci step halt || echo "Skipping parameter received:  << parameters.skip >>"
